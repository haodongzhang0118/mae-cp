# MAE-CP å¿«é€Ÿå…¥é—¨æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. å®‰è£…ä¾èµ–

```bash
# è¿›å…¥ stable-pretraining ç›®å½•
cd /Users/zhanghaodong/Desktop/DIET-CP/DINOv3-CP/dinov3/stable-pretraining

# å®‰è£… stable-pretraining
pip install -e ".[vision,tracking]"

# å®‰è£…å…¶ä»–ä¾èµ–
pip install transformers datasets==2.20.0
```

### 2. æµ‹è¯•å®‰è£…

```bash
cd /Users/zhanghaodong/Desktop/DIET-CP/DINOv3-CP/dinov3
python mae_cp/test_mae_cp.py
```

### 3. è¿è¡Œå•ä¸ªå®éªŒ

```bash
python mae_cp/mae_cp_train.py \
    --dataset bloodmnist \
    --data_root /root/data \
    --limit_data 100 \
    --model_size base \
    --pretrained \
    --batch_size 64 \
    --epochs 10 \
    --output_dir /root/output/mae_cp_test
```

### 4. æ‰¹é‡å®éªŒ

```bash
# ç¼–è¾‘é…ç½®
vim mae_cp/run_mae_cp_experiments.sh

# è¿è¡Œ
./mae_cp/run_mae_cp_experiments.sh
```

---

## ğŸ“ æ–‡ä»¶è¯´æ˜

| æ–‡ä»¶ | åŠŸèƒ½ | é‡è¦æ€§ |
|------|------|--------|
| `load_mae_weights.py` | ä» HuggingFace åŠ è½½é¢„è®­ç»ƒ MAE æƒé‡ | â­â­â­ |
| `mae_cp_dataset.py` | æ•°æ®é›†é€‚é…å™¨ï¼ˆCPDataset â†’ stable-pretrainingï¼‰ | â­â­â­ |
| `mae_cp_train.py` | ä¸»è®­ç»ƒè„šæœ¬ | â­â­â­â­â­ |
| `run_mae_cp_experiments.sh` | æ‰¹é‡å®éªŒè„šæœ¬ | â­â­â­â­ |
| `test_mae_cp.py` | ç»„ä»¶æµ‹è¯•è„šæœ¬ | â­â­ |
| `README.md` | è¯¦ç»†æ–‡æ¡£ | â­â­ |
| `QUICKSTART.md` | æœ¬æ–‡ä»¶ | â­â­ |

---

## ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ

### 1. **HuggingFace æƒé‡åŠ è½½**

```python
from load_mae_weights import load_pretrained_mae_weights
import stable_pretraining as spt

# åˆ›å»º MAE æ¨¡å‹
mae_model = spt.backbone.mae.vit_base_patch16_dec512d8b()

# ä» HuggingFace åŠ è½½æƒé‡
load_pretrained_mae_weights(
    mae_model,
    source="facebook/vit-mae-base",  # HuggingFace model ID
    strict=False
)
```

**æ”¯æŒçš„é¢„è®­ç»ƒæ¨¡å‹ï¼š**
- `facebook/vit-mae-base` - ViT-Base (768-dim, 12 layers)
- `facebook/vit-mae-large` - ViT-Large (1024-dim, 24 layers)
- `facebook/vit-mae-huge` - ViT-Huge (1280-dim, 32 layers)

### 2. **æ•°æ®é›†é€‚é…**

```python
from mae_cp_dataset import MAE_CPDataset

# åˆ›å»ºæ•°æ®é›†ï¼ˆè‡ªåŠ¨é€‚é… CPDatasetï¼‰
dataset = MAE_CPDataset(
    dataset_name="bloodmnist",
    root="/root/data/bloodmnist",
    split="TRAIN",
    limit_data=100,  # Few-shot
    transform=transform,
)

# è¿”å›æ ¼å¼ï¼š{"image": Tensor, "label": int}
```

### 3. **è®­ç»ƒæµç¨‹**

```python
from mae_cp_train import mae_cp_forward
import stable_pretraining as spt

# 1. åˆ›å»º MAE backbone
backbone = spt.backbone.mae.vit_base_patch16_dec512d8b()

# 2. åŠ è½½é¢„è®­ç»ƒæƒé‡
load_pretrained_mae_weights(backbone, "facebook/vit-mae-base")

# 3. åˆ›å»º Module
module = spt.Module(
    backbone=backbone,
    forward=mae_cp_forward,  # è‡ªå®šä¹‰ forward å‡½æ•°
    optim={...},
)

# 4. åˆ›å»º Trainer å¹¶è®­ç»ƒ
trainer = pl.Trainer(...)
manager = spt.Manager(trainer=trainer, module=module, data=data)
manager()
```

### 4. **Forward å‡½æ•°é€»è¾‘**

```python
def mae_cp_forward(self, batch, stage):
    # 1. MAE forward pass
    latent, pred, mask = self.backbone(batch["image"])
    
    # 2. æå– CLS token ä½œä¸º embedding
    out = {"embedding": latent[:, 0]}
    
    # 3. è®­ç»ƒæ—¶è®¡ç®—é‡å»ºæŸå¤±
    if self.training:
        target = self.backbone.patchify(batch["image"])
        loss = spt.losses.mae(target, pred, mask)
        out["loss"] = loss
    
    return out
```

---

## ğŸ¯ å…¸å‹ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1: Few-shot å®éªŒï¼ˆå°‘æ ·æœ¬ï¼‰

```bash
for num_samples in 10 50 100 250 500; do
    python mae_cp/mae_cp_train.py \
        --dataset food101 \
        --limit_data $num_samples \
        --epochs 100 \
        --output_dir /root/output/mae_cp/food101_fewshot
done
```

### åœºæ™¯ 2: å¤šæ•°æ®é›†å¯¹æ¯”

```bash
for dataset in bloodmnist pathmnist chestmnist; do
    python mae_cp/mae_cp_train.py \
        --dataset $dataset \
        --epochs 100 \
        --output_dir /root/output/mae_cp/medmnist_comparison
done
```

### åœºæ™¯ 3: ä¸åŒæ¨¡å‹è§„æ ¼å¯¹æ¯”

```bash
for model_size in base large huge; do
    python mae_cp/mae_cp_train.py \
        --dataset food101 \
        --model_size $model_size \
        --output_dir /root/output/mae_cp/model_size_comparison
done
```

---

## ğŸ”§ å¸¸è§é…ç½®

### å°å†…å­˜é…ç½®ï¼ˆ<16GB GPUï¼‰

```bash
python mae_cp/mae_cp_train.py \
    --dataset bloodmnist \
    --model_size base \
    --batch_size 64 \
    --precision 16-mixed \
    --num_workers 4
```

### å¤š GPU è®­ç»ƒ

```bash
python mae_cp/mae_cp_train.py \
    --dataset food101 \
    --devices 4 \
    --batch_size 256
```

### å¿«é€Ÿæµ‹è¯•ï¼ˆéªŒè¯æµç¨‹ï¼‰

```bash
python mae_cp/mae_cp_train.py \
    --dataset bloodmnist \
    --limit_data 100 \
    --epochs 5 \
    --batch_size 32 \
    --num_workers 0
```

---

## ğŸ“Š ç›‘æ§è®­ç»ƒ

### 1. **ä½¿ç”¨ W&B**

```bash
python mae_cp/mae_cp_train.py \
    --dataset food101 \
    --use_wandb \
    --wandb_project mae-cp-food101
```

ç„¶åè®¿é—®ï¼šhttps://wandb.ai/your-username/mae-cp-food101

### 2. **ä½¿ç”¨ TensorBoard**

```bash
# è®­ç»ƒä¼šè‡ªåŠ¨ä¿å­˜ CSV logs
tensorboard --logdir /root/output/mae_cp
```

### 3. **æŸ¥çœ‹å®æ—¶æŒ‡æ ‡**

è®­ç»ƒè¿‡ç¨‹ä¸­ä¼šæ˜¾ç¤ºï¼š
- `train/loss`: é‡å»ºæŸå¤±
- `val/linear_probe/top1`: Linear probe å‡†ç¡®ç‡ï¼ˆåœ¨çº¿ç›‘æ§ï¼‰
- `val/rankme/rankme`: è¡¨ç¤ºè´¨é‡æŒ‡æ ‡

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: ImportError - transformers

```bash
pip install transformers
```

### é—®é¢˜ 2: Out of Memory

å‡å° batch size:
```bash
--batch_size 64  # æˆ– 32, 16
```

### é—®é¢˜ 3: æ•°æ®é›†æ‰¾ä¸åˆ°

ç¡®ä¿æ•°æ®è·¯å¾„æ­£ç¡®ï¼š
```bash
ls /root/data/bloodmnist  # åº”è¯¥æœ‰æ•°æ®æ–‡ä»¶
```

æˆ–ä¿®æ”¹ `--data_root`:
```bash
--data_root /path/to/your/data
```

### é—®é¢˜ 4: é¢„è®­ç»ƒæƒé‡ä¸‹è½½æ…¢

è®¾ç½® HuggingFace é•œåƒï¼š
```bash
export HF_ENDPOINT=https://hf-mirror.com
```

æˆ–ä½¿ç”¨æœ¬åœ°æƒé‡ï¼š
```bash
--pretrained_source /path/to/mae_checkpoint.pth
```

### é—®é¢˜ 5: stable-pretraining æ‰¾ä¸åˆ°

ç¡®ä¿æ·»åŠ åˆ° PYTHONPATH:
```bash
export PYTHONPATH="/path/to/stable-pretraining:$PYTHONPATH"
```

---

## ğŸ“ˆ é¢„æœŸç»“æœ

### Few-shot Learningï¼ˆBloodMNISTï¼‰

| Samples | Random Init | MAE-CP (Base) | Improvement |
|---------|-------------|---------------|-------------|
| 10 | 20.3% | 42.1% | +21.8% |
| 50 | 45.2% | 68.4% | +23.2% |
| 100 | 58.7% | 76.3% | +17.6% |
| Full | 84.2% | 91.7% | +7.5% |

*ä»¥ä¸Šä¸ºç¤ºä¾‹æ•°æ®ï¼Œå®é™…ç»“æœå¯èƒ½æœ‰æ‰€ä¸åŒ*

### è®­ç»ƒæ—¶é—´ï¼ˆViT-Base, 1x A100ï¼‰

| Dataset | Samples | Epochs | Time |
|---------|---------|--------|------|
| BloodMNIST | 100 | 100 | ~15 min |
| Food101 | 1000 | 100 | ~2 hours |
| Food101 | Full (75K) | 100 | ~20 hours |

---

## ğŸ”— ç›¸å…³èµ„æº

- **MAE è®ºæ–‡**: https://arxiv.org/abs/2111.06377
- **stable-pretraining æ–‡æ¡£**: https://rbalestr-lab.github.io/stable-pretraining/
- **HuggingFace MAE**: https://huggingface.co/facebook/vit-mae-base
- **DINOv3-CP (å¯¹æ¯”)**: ../dinov3/configs/train/cp/

---

## ğŸ’¡ æœ€ä½³å®è·µ

1. **å…ˆç”¨å°æ•°æ®æµ‹è¯•**: ä½¿ç”¨ `--limit_data 100 --epochs 5` éªŒè¯æµç¨‹
2. **ç›‘æ§ Linear Probe**: å¦‚æœ probe å‡†ç¡®ç‡ä¸æå‡ï¼Œè¯´æ˜è¡¨ç¤ºè´¨é‡æ²¡æœ‰æ”¹å–„
3. **ä½¿ç”¨é¢„è®­ç»ƒæƒé‡**: `--pretrained` å‡ ä¹æ€»æ˜¯èƒ½æä¾›æ›´å¥½çš„èµ·ç‚¹
4. **è°ƒæ•´ learning rate**: å¦‚æœ loss ä¸ä¸‹é™ï¼Œå°è¯•é™ä½ `--lr`ï¼ˆå¦‚ 1e-4ï¼‰
5. **ä¿å­˜ checkpoints**: å®šæœŸå¤‡ä»½ `/root/output/mae_cp`

---

## ğŸ“ è¿›é˜¶ä½¿ç”¨

### è‡ªå®šä¹‰ Forward å‡½æ•°

ç¼–è¾‘ `mae_cp_train.py` ä¸­çš„ `mae_cp_forward` å‡½æ•°æ¥å®ç°è‡ªå®šä¹‰è®­ç»ƒé€»è¾‘ã€‚

### æ·»åŠ æ–°æ•°æ®é›†

1. åœ¨ `dinov3/data/datasets/cp_datasets.py` ä¸­æ·»åŠ æ•°æ®é›†æ”¯æŒ
2. åœ¨ `DATASET_STATS` ä¸­æ·»åŠ ç»Ÿè®¡ä¿¡æ¯
3. ä½¿ç”¨ `MAE_CPDataset` è‡ªåŠ¨é€‚é…

### ä¿®æ”¹ä¼˜åŒ–å™¨

```python
module = spt.Module(
    ...,
    optim={
        "optimizer": {
            "type": "AdamW",
            "lr": 1e-4,
            "weight_decay": 0.05,
            "betas": (0.9, 0.95),
        },
        "scheduler": {
            "type": "CosineAnnealingLR",
            "T_max": epochs,
        },
    },
)
```

---

## âœ… Checklist

å¼€å§‹è®­ç»ƒå‰çš„æ£€æŸ¥æ¸…å•ï¼š

- [ ] å®‰è£…äº† `stable-pretraining` å’Œä¾èµ–
- [ ] æµ‹è¯•è„šæœ¬ `test_mae_cp.py` é€šè¿‡
- [ ] æ•°æ®é›†å·²ä¸‹è½½åˆ°æ­£ç¡®è·¯å¾„
- [ ] GPU å¯ç”¨ï¼ˆ`torch.cuda.is_available()`ï¼‰
- [ ] è¶³å¤Ÿçš„ç£ç›˜ç©ºé—´ï¼ˆè‡³å°‘ 50GBï¼‰
- [ ] é…ç½®äº† HuggingFace tokenï¼ˆå¦‚éœ€ä¸‹è½½æ¨¡å‹ï¼‰

---

## ğŸ™‹ è·å–å¸®åŠ©

1. **æŸ¥çœ‹è¯¦ç»†æ–‡æ¡£**: `mae_cp/README.md`
2. **è¿è¡Œæµ‹è¯•**: `python mae_cp/test_mae_cp.py`
3. **æŸ¥çœ‹ç¤ºä¾‹**: `mae_cp/mae_cp_train.py` çš„ `__main__` éƒ¨åˆ†
4. **å¯¹æ¯” DINOv3-CP**: å‚è€ƒ `run_all_experiments.txt`

ç¥è®­ç»ƒé¡ºåˆ©ï¼ğŸš€

